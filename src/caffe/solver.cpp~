#include <cstdio>
#include <stdio.h>
#include <string>
#include <vector>
#include "caffe/util/format.hpp"
#include "caffe/solver.hpp"

#include "caffe/util/io.hpp"


namespace caffe {

template <typename Dtype>
Solver<Dtype>::Solver(const SolverParameter& param)
{
  //---------------- initial train net -----------------------
  Caffe::set_phase(TRAIN);
  param_ = param;
  NetParameter net_param;
  ReadProtoFromTextFile(param_.net(), &net_param);
  net_.reset(new Net<Dtype>(net_param));
  iter_ = 0;
	current_step_ = 0;

  const vector<shared_ptr<Blob<Dtype> > >& net_params = this->net_->params();
  this->history_.clear();
  for (int i = 0; i < net_params.size(); ++i)
  {
    int num = net_params[i]->num();
    int channels = net_params[i]->channels();
    int height = net_params[i]->height();
    int width = net_params[i]->width();
    this->history_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(num,channels,height,width)));
    this->update_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(num,channels,height,width)));
    this->temp_.push_back(shared_ptr<Blob<Dtype> >(new Blob<Dtype>(num,channels,height,width)));
  }
  //---------------- initial test net -----------------------
  if (param_.has_test_initialization())
  {
    Caffe::set_phase(TEST);
    test_net_.reset(new Net<Dtype>(net_param));
  }
}

template <typename Dtype>
void Solver<Dtype>::Solve(const char* resume_file) 
{
  LOG(INFO) << "Solving " << net_->name();
  LOG(INFO) << "Learning Rate Policy: " << param_.lr_policy();


  if (resume_file)
  {
    LOG(INFO) << "Restoring previous solver status from " << resume_file;
    Restore(resume_file);
  }

  start_iter_ = iter_;
  average_loss_ = this->param_.average_loss();
  losses_.clear();
  smoothed_loss_ = 0;

  if (param_.has_test_initialization() && param_.test_initialization())
  {
    //-------------- copy net-------------------
    for (int i = 0; i < test_net_->params().size(); i++)
      test_net_->params()[i]->set_data(*net_->params()[i]);
    //---------------- forward test --------------
    Caffe::set_phase(TEST);
    if (param_.eval_type() == "classification")
    	test();
    else if (param_.eval_type() == "segmentation")
    	testSegmentation();
    else
    	LOG(FATAL)<<" unrecognized mode for test";
  }

  while (iter_ < param_.max_iter())
  {

    Caffe::set_phase(TRAIN);
    net_->ClearParamDiffs();
    Dtype loss = 0;
    for (int i = 0; i < param_.iter_size(); ++i)
    {

      loss += net_->Forward();
      net_->Backward();
    }
    loss /= param_.iter_size();

    dispaly_loss(loss);
		
    ApplyUpdate();
    ++iter_;
    
    if (param_.has_test_interval() && iter_ % param_.test_interval() == 0)
    {
    
    	if (param_.accumulate_batch_norm())
			{
				LOG(INFO)<<"==accumulate statisticals of samples for batch norm====";
				Caffe::set_phase(TRAIN);
				Caffe::number_collect_sample = 0;
				for (int i = 0; i < param_.accumulate_test_iter(); ++i)
				{
					net_->Forward();
					Caffe::number_collect_sample ++;
				}	
				Caffe::number_collect_sample = -1;
			}
    	
    	LOG(INFO)<<"===== test the model ======";
      //-------------- copy net-------------------
      for (int i = 0; i < test_net_->params().size(); i++)
        test_net_->params()[i]->set_data(*net_->params()[i]);
      //---------------- forward test --------------
      Caffe::set_phase(TEST);
      if (param_.eval_type() == "classification")
      	test();
      else if (param_.eval_type() == "segmentation")
      	testSegmentation();
      else
      	LOG(FATAL)<<" unrecognized mode for test";
    }
    if (param_.snapshot() && iter_ % param_.snapshot() == 0 )
      Snapshot();
  }
  //at the end of the training, accumulate the batch norm statisticals 
  if (param_.accumulate_batch_norm())
  {
  	LOG(INFO)<<"====accumulate statisticals of samples for batch norm=====";
  	Caffe::set_phase(TRAIN);
  	Caffe::number_collect_sample = 0;
  	for (int i = 0; i < param_.accumulate_max_iter(); ++i)
		{
			net_->Forward();
			Caffe::number_collect_sample ++;
		}	
		Caffe::number_collect_sample = -1;
  }
	//at the end of the training, run one test
	if (param_.has_test_interval())
  {
  	LOG(INFO)<<"===== test the model ======";
    //-------------- copy net-------------------
    for (int i = 0; i < test_net_->params().size(); i++)
      test_net_->params()[i]->set_data(*net_->params()[i]);
    //---------------- forward test --------------
    Caffe::set_phase(TEST);
    if (param_.eval_type() == "classification")
    	test();
    else if (param_.eval_type() == "segmentation")
    	testSegmentation();
    else
    	LOG(FATAL)<<" unrecognized mode for test";
    
  }
  
	//Finally, save the model 
	Snapshot();
	
  LOG(INFO) << "Optimization Done.";
}


template <typename Dtype>
void Solver<Dtype>::dispaly_loss(Dtype loss) 
{
  const bool display = param_.display() && iter_ % param_.display() == 0;


  if (losses_.size() < average_loss_)
  {
    losses_.push_back(loss);
    int size = losses_.size();
    smoothed_loss_ = (smoothed_loss_ * (size - 1) + loss) / size;
  }
  else
  {
    int idx = (iter_ - start_iter_) % average_loss_;
    smoothed_loss_ += (loss - losses_[idx]) / average_loss_;
    losses_[idx] = loss;
  }
  if (display)
  {
    LOG(INFO) << "Iteration " << iter_  << ", loss = " << smoothed_loss_;
    const vector<Blob<Dtype>*>& result = net_->output_blobs();
    int score_index = 0;
    for (int j = 0; j < result.size(); ++j)
    {
      const Dtype* result_vec = result[j]->cpu_data();
      const string& output_name = net_->blob_names()[net_->output_blob_indices()[j]];
      const Dtype loss_weight = net_->blob_loss_weights()[net_->output_blob_indices()[j]];
      for (int k = 0; k < result[j]->count(); ++k)
      {
        ostringstream loss_msg_stream;
        if (loss_weight)
          loss_msg_stream << " (* " << loss_weight << " = " << loss_weight * result_vec[k] << " loss)";

        LOG(INFO)<<"Train net output #" <<score_index++<< ": " <<output_name<< " = " <<result_vec[k]<<loss_msg_stream.str();
      }
    }
  }
}
template <typename Dtype>
void Solver<Dtype>::test() 
{
  vector<int> test_score_output_id;
  vector<float> test_score;
  float loss = 0;
  for (int i = 0; i < param_.test_iter(); i++)
  {
    float iter_loss = test_net_->Forward();
    loss += iter_loss;
    int idx = 0;
    for (int j = 0; j < test_net_->output_blobs().size(); ++j)
    {
      for (int k = 0; k < test_net_->output_blobs()[j]->count(); ++k, ++idx)
      {
        const float score = test_net_->output_blobs()[j]->cpu_data()[k];
        if (i == 0)
        {
          test_score.push_back(score);
          test_score_output_id.push_back(j);
        }
        else
          test_score[idx] += score;

        const std::string& output_name = test_net_->blob_names()[test_net_->output_blob_indices()[j]];
        //LOG(INFO) << "Batch " << i << ", " << output_name << " = " << score;
      }
    }
  }

  loss /= param_.test_iter();
  LOG(INFO) << "Loss: " << loss;
  for (int i = 0; i < test_score.size(); ++i)
  {
    const std::string& output_name = test_net_->blob_names()[test_net_->output_blob_indices()[test_score_output_id[i]]];
    const float loss_weight = test_net_->blob_loss_weights()[test_net_->output_blob_indices()[test_score_output_id[i]]];
    std::ostringstream loss_msg_stream;
    const float mean_score = test_score[i] / param_.test_iter();
    if (loss_weight)
      loss_msg_stream << " (* " << loss_weight<< " = " << loss_weight * mean_score << " loss)";

    LOG(INFO) << output_name << " = " << mean_score << loss_msg_stream.str();
  }
}

template <typename Dtype>
void Solver<Dtype>::testSegmentation() 
{
  LOG(INFO) << "Iteration " << iter_ << ", Testing net ";

  vector<shared_ptr<Blob<Dtype> > > label_stats;
  Dtype loss = 0;
  for (int i = 0; i < param_.test_iter(); i++) 
  {
    Dtype iter_loss = test_net_->Forward();
    loss += iter_loss;
    
    if (test_net_->output_blobs().size() == 0)
      continue;

    if (i == 0) 
    {
      for (int j = 0; j < test_net_->output_blobs().size(); ++j)
      {
        shared_ptr<Blob<Dtype> > label_stat(new Blob<Dtype>());
        label_stats.push_back(label_stat);
        label_stat->Reshape(1, test_net_->output_blobs()[j]->channels(), test_net_->output_blobs()[j]->height(), test_net_->output_blobs()[j]->width());
        caffe_copy(test_net_->output_blobs()[j]->count(), test_net_->output_blobs()[j]->cpu_data(), label_stat->mutable_cpu_data());
      }
    } 
    else 
    {
      // add the result
      CHECK_EQ(test_net_->output_blobs().size(),1);
      for (int j = 0; j < test_net_->output_blobs().size(); ++j)
      {         
        caffe_axpy(test_net_->output_blobs()[j]->count(), 
        					Dtype(1), test_net_->output_blobs()[j]->cpu_data(), 
        					label_stats[j]->mutable_cpu_data());
      }
    }    
  }

  loss /= param_.test_iter();
  LOG(INFO) << "Test loss: " << loss;

  for (int i = 0; i < label_stats.size(); ++i) 
  {
    const int output_blob_index = test_net_->output_blob_indices()[i];
    const string& output_name = test_net_->blob_names()[output_blob_index];
    const Dtype* label_stat_data = label_stats[i]->cpu_data();
    const int channels = label_stats[i]->channels();
    // get sum infomation
    Dtype sum_gtpred = 0;
    Dtype sum_gt = 0;
    for (int c = 0; c < channels; ++c) 
    {
      sum_gtpred += label_stat_data[c*3];
      sum_gt += label_stat_data[c*3+1];
    }
    if (sum_gt > 0) 
    {
      Dtype per_pixel_acc = sum_gtpred / sum_gt;
      Dtype per_label_acc = 0, iou, iou_acc = 0, weighted_iou_acc = 0;
      int num_valid_labels = 0;
      for (int c = 0; c < channels; ++c) 
      {
        if (label_stat_data[1] != 0) 
        {
          per_label_acc += label_stat_data[0] / label_stat_data[1];
          ++num_valid_labels;
        }
        if (label_stat_data[1] + label_stat_data[2] != 0) 
        {
          iou = label_stat_data[0] / (label_stat_data[1] + label_stat_data[2] - label_stat_data[0]);
          iou_acc += iou;
          weighted_iou_acc += iou * label_stat_data[1] / sum_gt;
        }
        label_stat_data += label_stats[i]->offset(0, 1);
      }
      LOG(INFO) << "    Test net output #" << i << " " << output_name << ": per_pixel_acc = " << per_pixel_acc;
      LOG(INFO) << "    Test net output #" << i << " " << output_name << ": per_label_acc = " << per_label_acc / num_valid_labels;
      LOG(INFO) << "    Test net output #" << i << " " << output_name << ": iou_acc = " << iou_acc / num_valid_labels;
      LOG(INFO) << "    Test net output #" << i << " " << output_name << ": weighted_iou_acc = " << weighted_iou_acc;
    } 
    else 
      LOG(INFO) << "    Test net output #" << i << " " << output_name << ": no valid labels!";
    
  }
}

///------------------------------------------------ proto <->  memory------------------------------------------------------
template <typename Dtype>
void Solver<Dtype>::Snapshot() 
{
  NetParameter net_param;
  net_->ToProto(&net_param);

  string model_filename = param_.snapshot_prefix() + "_iter_" +format_int(iter_) + ".caffemodel";
  LOG(INFO) << "Snapshotting to binary proto file " << model_filename;
  WriteProtoToBinaryFile(net_param, model_filename);

  SolverState state;
  state.set_iter(this->iter_);
  state.set_learned_net(model_filename);
  state.clear_history();
  for (int i = 0; i < history_.size(); ++i)
  {
    BlobProto* history_blob = state.add_history();
    history_[i]->ToProto(history_blob);
  }

  string snapshot_filename = param_.snapshot_prefix() + "_iter_"+ format_int(iter_)+ ".solverstate";
  LOG(INFO) << "Snapshotting solver state to binary proto file " << snapshot_filename;
  WriteProtoToBinaryFile(state, snapshot_filename.c_str());
}


template <typename Dtype>
void Solver<Dtype>::Restore(const char* state_file) 
{
  string state_filename(state_file);
  SolverState state;
  ReadProtoFromBinaryFile(state_file, &state);
  this->iter_ = state.iter();
  for (int i = 0; i < history_.size(); ++i)
    history_[i]->FromProto(state.history(i));

  NetParameter net_param;
  ReadProtoFromBinaryFile(state.learned_net().c_str(), &net_param);
  this->net_->CopyTrainedLayersFrom(net_param);
  //------------------------------------------------------------------------------------------------------------------------
}

INSTANTIATE_CLASS(Solver);

}  // namespace caffe
