// Copyright Yangqing Jia 2013

#include <map>
#include <set>
#include <string>
#include <vector>

#include "caffe/proto/caffe.pb.h"
#include "caffe/layer_factory.hpp"
#include "caffe/net.hpp"

using std::pair;
using std::map;
using std::set;

namespace caffe {

template <typename Dtype>
Net<Dtype>::Net(const NetParameter& param):param_(param)
{
  // Basically, build all the layers and set up its connections.
  /*-------------                    ----------------*/
  param_.clear_layer();
  for (int i = 0; i < param.layer_size(); i++ )
  {
    if (param.layer(i).include().has_phase() && param.layer(i).include().phase() != Caffe::phase())
      continue;
    else
      param_.add_layer()->CopyFrom(param.layer(i));
  }
  /*-------------------------------------------------*/
  
  
  
  name_ = param_.name();
  map<string, int> blob_name_to_idx;
  set<string> available_blobs;


  blobs_.clear();
  blob_names_.clear();
  blob_loss_weights_.clear();
	
	input_blobs_.clear();
  input_blob_indices_.clear();
  available_blobs.clear();
  blob_name_to_idx.clear();
  //---------------------------------------------- get the input blob ------------------------------------------
  for (int i = 0; i < param_.input_blob_size(); ++i)
  {
    const string& blob_name = param_.input_blob(i).name();
    int num = param_.input_blob(i).num();
    int channels = param_.input_blob(i).channels();
    int height = param_.input_blob(i).height();
    int width = param_.input_blob(i).width();

    shared_ptr<Blob<Dtype> > blob_pointer(new Blob<Dtype>(num, channels, height, width));
    blobs_.push_back(blob_pointer);
    blob_names_.push_back(blob_name);
    blob_loss_weights_.push_back(Dtype(0));
    blob_name_to_idx[blob_name] = i;
		
		input_blobs_.push_back(blob_pointer.get());
    input_blob_indices_.push_back(i);
    available_blobs.insert(blob_name);
  }
  //------------------------------------------------- build top and bottom ---------------------------------------------
  bottom_vecs_.resize(param_.layer_size());
  top_vecs_.resize(param_.layer_size());
  bottom_id_vecs_.resize(param_.layer_size());
  top_id_vecs_.resize(param_.layer_size());

  for (int i = 0; i < param_.layer_size(); ++i)
  {
    const LayerParameter& layer_param = param_.layer(i);
    layers_.push_back(LayerRegistry<Dtype>::CreateLayer(layer_param));
    vector<string>::iterator iter = find(layer_names_.begin(),layer_names_.end(),layer_param.name());
    if (iter != layer_names_.end())
    		LOG(FATAL)<<"Duplicate layer name "<<layer_param.name();
    layer_names_.push_back(layer_param.name());
    layer_need_backward_.push_back(layer_param.include().need_backward());
    LOG(INFO) << "Creating Layer " << layer_param.name();
    for (int j = 0; j < layer_param.bottom_size(); ++j)
    {
      const string& blob_name = layer_param.bottom(j);
      if (available_blobs.find(blob_name) == available_blobs.end())
        LOG(FATAL) << "Unknown blob input " << blob_name << " to layer" << j;

      LOG(INFO) << layer_param.name() << " <- " << blob_name;
      bottom_vecs_[i].push_back(blobs_[blob_name_to_idx[blob_name]].get());
      bottom_id_vecs_[i].push_back(blob_name_to_idx[blob_name]);
      available_blobs.erase(blob_name);
    }
    for (int j = 0; j < layer_param.top_size(); ++j)
    {
      const string& blob_name = layer_param.top(j);
      bool in_place = false;
      if (blob_name_to_idx.find(blob_name) != blob_name_to_idx.end())
      {
        for (int k=0;k<layer_param.bottom_size();k++)
        {
          if (layer_param.bottom(k) == blob_name)
          {
            in_place = true;
            break;
          }
        }
        if (!in_place)
          LOG(FATAL) << "Duplicate blobs "<<blob_name<<" produced by multiple sources.";
      }


      LOG(INFO) << layer_param.name() << " -> " << blob_name;
      if (in_place)
      {
        //-------------previous blob-----------------------
        available_blobs.insert(blob_name);
        top_vecs_[i].push_back(blobs_[blob_name_to_idx[blob_name]].get());
        top_id_vecs_[i].push_back(blob_name_to_idx[blob_name]);
      }
      else
      {
        //-------------new data blob----------------------
        shared_ptr<Blob<Dtype> > blob_pointer(new Blob<Dtype>());
        blobs_.push_back(blob_pointer);
        blob_names_.push_back(blob_name);
        blob_loss_weights_.push_back(layer_param.include().loss_weight());
        blob_name_to_idx[blob_name] = blob_names_.size() - 1;

        available_blobs.insert(blob_name);
        top_vecs_[i].push_back(blobs_[blob_name_to_idx[blob_name]].get());
        top_id_vecs_[i].push_back(blob_name_to_idx[blob_name]);
      }
    }
  }

  //------------------------------------------------- set up layers ---------------------------------------------
  for (set<string>::iterator it = available_blobs.begin(); it != available_blobs.end(); ++it)
  {
    LOG(ERROR) << "This network produces output " << *it;
    output_blob_indices_.push_back(blob_name_to_idx[*it]);
    output_blobs_.push_back(blobs_[blob_name_to_idx[*it]].get());
  }

  LOG(ERROR) << "Setting up the layers.";
  for (int i = 0; i < layers_.size(); ++i)
  {
    LOG(INFO) << "Setting up " << layer_names_[i];
    layers_[i]->SetUp(bottom_vecs_[i], top_vecs_[i]);
    vector<shared_ptr<Blob<Dtype> > >& layer_blobs = layers_[i]->blobs();
    for (int j = 0; j < layer_blobs.size(); ++j)
    {
      params_.push_back(layer_blobs[j]);
      if (layer_need_backward_[i])
      	params_lr_.push_back(layers_[i]->lr_mult()[j]);
      else
      	params_lr_.push_back(Dtype(0));
      params_weight_decay_.push_back(layers_[i]->decay_mult()[j]);
    }
    for (int topid = 0; topid < top_vecs_[i].size(); ++topid)
      LOG(INFO) << "Top shape: " << top_vecs_[i][topid]->num()
                << " "  << top_vecs_[i][topid]->channels()
                << " " << top_vecs_[i][topid]->height()
                << " "  << top_vecs_[i][topid]->width();

  }
//----------------------------------------------
LOG(INFO)<<"*************************** computing memory ***************************************";
	
	Dtype memory_cost_data = 0;
	Dtype max_diff = 0;
	for (int i=0;i<blobs_.size();i++)
	{
		memory_cost_data += blobs_[i]->count();
		if (max_diff < blobs_[i]->count())
			 max_diff = blobs_[i]->count();
	}	
	LOG(INFO)<<"Data cost memory is "<<memory_cost_data/(1024*1024*1024)*sizeof(Dtype)<<"GB";
	LOG(INFO)<<"Diff cost memory is "<<max_diff*param_.num_flow()/(1024*1024*1024)*sizeof(Dtype)<<"GB";
	Dtype memory_cost_weight = 0;
	for (int i=0;i<params_.size();i++)
		memory_cost_weight += params_[i]->count();
	LOG(INFO)<<"Weight cost memory is "<<memory_cost_weight*3/(1024*1024*1024)*sizeof(Dtype)<<"GB";
	
	LOG(INFO)<<"Totol memeory cost is "<<(memory_cost_data+max_diff*param_.num_flow()+memory_cost_weight*3)*sizeof(Dtype)/(1024*1024*1024)<<"GB";
LOG(INFO)<<"*************************************************************************************";
//----------------------------------------------
  LOG(ERROR) << "Network initialization done.";
  if (param_.num_flow() > 0)
  {
		tensor_flows_.resize(param_.num_flow());
		tensor_flows_temp_.resize(param_.num_flow());
		for (int i=0;i<tensor_flows_.size();i++)
		{
		  tensor_flows_[i].reset(new Blob<Dtype>(1,1,1,1));
		  tensor_flows_temp_[i].reset(new Blob<Dtype>(1,1,1,1));
		}
		flow_flag.resize(param_.num_flow(),true);
	}	
}

template <typename Dtype>
Dtype Net<Dtype>::Forward()
{
  Dtype loss=0;
  for (int i = 0; i < layers_.size(); ++i)
  {

   	//LOG(INFO)<<"fowarding layer "<<layer_names_[i];

    layers_[i]->Reshape(bottom_vecs_[i], top_vecs_[i]);     
    /*------------------------------------------------------------------------------*/
    if ((Caffe::phase() == TEST||!layer_need_backward_[i]) && param_.num_flow() > 0)
    {  	 
  		CHECK_EQ(param_.layer(i).bottom_flow_size(),bottom_vecs_[i].size());
  		CHECK_EQ(param_.layer(i).top_flow_size(),top_vecs_[i].size());
    	
      /*----- set the point to the flow------------ */
      for (int j=0;j<bottom_vecs_[i].size();j++)
      {
        int current_flow = param_.layer(i).bottom_flow(j);              
        if (current_flow < 0)
      		continue;
        
        CHECK_LE(current_flow,tensor_flows_.size()-1);
        
        if (flow_flag[current_flow])
          bottom_vecs_[i][j]->set_data(*tensor_flows_[current_flow]);
        else
          bottom_vecs_[i][j]->set_data(*tensor_flows_temp_[current_flow]);

      }


      //new head for the flow
      for (int j=0;j<top_vecs_[i].size();j++)
      {
        int current_flow;
        if (param_.layer(i).top_flow_size() > j)
        	current_flow = param_.layer(i).top_flow(j);
        else
          break;
        
        if (current_flow < 0)
      		continue;
        
        bool inplace = false;
        for (int k=0;k<param_.layer(i).bottom_size();k++)// check if in-place computation
        {
          if (param_.layer(i).top(j) == param_.layer(i).bottom(k))
          {
            if (flow_flag[current_flow])
              top_vecs_[i][j]->set_data(*tensor_flows_[current_flow]);
            else
              top_vecs_[i][j]->set_data(*tensor_flows_temp_[current_flow]);

            inplace = true;
            break;
          }
        }

        if (!inplace) // move the top to the other data blob
        {
          if (flow_flag[current_flow])
          {
            top_vecs_[i][j]->set_data(*tensor_flows_temp_[current_flow]);
            flow_flag[current_flow] = false;
          }
          else
          {
            top_vecs_[i][j]->set_data(*tensor_flows_[current_flow]);
            flow_flag[current_flow] = true;
          }
        }
      }
    }  
    /*------------------------------------------------------------------------------*/
    Dtype layer_loss = layers_[i]->Forward(bottom_vecs_[i], top_vecs_[i]);
    loss += layer_loss;
  }
  return loss;
}

template <typename Dtype>
void Net<Dtype>::Backward()
{
  for (int i = layers_.size() - 1; i >= 0; --i)
  {  	
  	//LOG(INFO)<<"backwarding layer "<<layer_names_[i];
  	
    if (!layer_need_backward_[i])
      continue;
    if (param_.num_flow() > 0)
    {
    	
  		CHECK_EQ(param_.layer(i).bottom_flow_size(),bottom_vecs_[i].size());
  		CHECK_EQ(param_.layer(i).top_flow_size(),top_vecs_[i].size());
    	
    	
/*--------------------- set the point of the gradient to the flow---------------------- */
		  for (int j=0;j<top_vecs_[i].size();j++)
		  {
		  	int current_flow = param_.layer(i).top_flow(j);	    
		    if (current_flow < 0)
		    	continue;

		    CHECK_LE(current_flow,tensor_flows_.size()-1);
		    
		    if (flow_flag[current_flow])
		      top_vecs_[i][j]->set_diff(*tensor_flows_[current_flow]);
		    else
		      top_vecs_[i][j]->set_diff(*tensor_flows_temp_[current_flow]);
		  }
		  //new head for the gradient flow
		  for (int j=0;j<bottom_vecs_[i].size();j++)
		  {
		    int current_flow;
		    if (param_.layer(i).bottom_flow_size() > j)
		    	current_flow = param_.layer(i).bottom_flow(j);
		    else	
		      break;
		    
		    if (current_flow < 0)
		    	continue;

		    bool inplace = false;
		    for (int k=0;k<param_.layer(i).top_size();k++)// check if in-place computation
		    {
		      if (param_.layer(i).bottom(j) == param_.layer(i).top(k))
		      {
		        if (flow_flag[current_flow])
		          bottom_vecs_[i][j]->set_diff(*tensor_flows_[current_flow]);
		        else
		          bottom_vecs_[i][j]->set_diff(*tensor_flows_temp_[current_flow]);

		        inplace = true;
		        break;
		      }
		    }

		    if (!inplace) // move the bottom to the other gradient blob
		    {
		      if (flow_flag[current_flow])
		      {
		        bottom_vecs_[i][j]->set_diff(*tensor_flows_temp_[current_flow]);
		        flow_flag[current_flow] = false;
		      }
		      else
		      {
		        bottom_vecs_[i][j]->set_diff(*tensor_flows_[current_flow]);
		        flow_flag[current_flow] = true;
		      }
		    }
		  }
/* ------------------------------------------------------------------------------------ */          
		} 
    layers_[i]->Backward(top_vecs_[i], bottom_vecs_[i]);
  }  
}


//----------------------------------------- proto <->  memory--------------------
template <typename Dtype>
void Net<Dtype>::CopyTrainedLayersFrom(const NetParameter& param)
{

  int num_source_layers = param.layer_size();
  vector<bool> layer_init_flag(layer_names_.size(), false);
  LOG(INFO)<<"............................... there are "<<num_source_layers<<".................................";
  for (int i = 0; i < num_source_layers; ++i)
  {
    const LayerParameter& source_layer = param.layer(i);
    const string& source_layer_name = source_layer.name();
    int target_layer_id = 0;
    while (target_layer_id != layer_names_.size() && (layer_names_[target_layer_id] != source_layer_name || layers_[target_layer_id]->blobs().size() !=  source_layer.blobs_size()))
      ++target_layer_id;

    if (target_layer_id == layer_names_.size())
    {
      LOG(INFO) << "Ignoring source layer " << source_layer_name;
      continue;
    }
    layer_init_flag[target_layer_id] = true;
    LOG(INFO) << "Loading source layer " << source_layer_name;
    vector<shared_ptr<Blob<Dtype> > >& target_blobs = layers_[target_layer_id]->blobs();
    CHECK_EQ(target_blobs.size(), source_layer.blobs_size()) << "Incompatible number of blobs for layer " << source_layer_name;
    
    for (int j = 0; j < target_blobs.size(); ++j)
    {
			if (source_layer.blobs(j).has_shape())
			{
				if (source_layer.blobs(j).shape().dim_size()==1)
					CHECK_EQ(target_blobs[j]->num(), source_layer.blobs(j).shape().dim(0));
				else if (source_layer.blobs(j).shape().dim_size()==2)
				{
					CHECK_EQ(target_blobs[j]->num(), source_layer.blobs(j).shape().dim(0));
					CHECK_EQ(target_blobs[j]->channels(), source_layer.blobs(j).shape().dim(1));
				}
				else if (source_layer.blobs(j).shape().dim_size()==3)
				{
					CHECK_EQ(target_blobs[j]->num(), source_layer.blobs(j).shape().dim(0));
					CHECK_EQ(target_blobs[j]->channels(), source_layer.blobs(j).shape().dim(1));
					CHECK_EQ(target_blobs[j]->height(), source_layer.blobs(j).shape().dim(2));
				}
				else if (source_layer.blobs(j).shape().dim_size()==4)
				{
					CHECK_EQ(target_blobs[j]->num(), source_layer.blobs(j).shape().dim(0));
					CHECK_EQ(target_blobs[j]->channels(), source_layer.blobs(j).shape().dim(1));
					CHECK_EQ(target_blobs[j]->height(), source_layer.blobs(j).shape().dim(2));
					CHECK_EQ(target_blobs[j]->width(), source_layer.blobs(j).shape().dim(3));
				}

			}
			else
			{
				CHECK_EQ(target_blobs[j]->num(), source_layer.blobs(j).num());
				CHECK_EQ(target_blobs[j]->channels(), source_layer.blobs(j).channels());
				CHECK_EQ(target_blobs[j]->height(), source_layer.blobs(j).height());
				CHECK_EQ(target_blobs[j]->width(), source_layer.blobs(j).width());
			}
      target_blobs[j]->FromProto(source_layer.blobs(j));
    }
  }
  for (int i = 0; i < layer_names_.size(); ++i) {
    if (!layer_init_flag[i] && layers_[i]->blobs().size()) {
      LOG(INFO) << "Target layer " << layer_names_[i] << " not initialized.";
    }
  }
}
template <typename Dtype>
void Net<Dtype>::ToProto(NetParameter* param, bool write_diff)
{
  param->Clear();
  param->set_name(name_);
  // Add bottom and top
  for (int i = 0; i < input_blob_indices_.size(); ++i)
    param->add_input(blob_names_[input_blob_indices_[i]]);

  LOG(INFO) << "Serializing " << layers_.size() << " layers";
  for (int i = 0; i < layers_.size(); ++i)
  {
    LayerParameter* layer_param = param->add_layer();
    layers_[i]->ToProto(layer_param, write_diff);
  }
}
//----------------------------------------------------------------------------

template <typename Dtype>
void Net<Dtype>::Update()
{
  for (int i = 0; i < params_.size(); ++i)
  {
#ifdef CPU_ONLY
    caffe_add(params_[i]->count(),params_[i]->cpu_data(),params_[i]->cpu_diff(),params_[i]->mutable_cpu_data());
#else
    caffe_gpu_add(params_[i]->count(), Dtype(1), params_[i]->gpu_data(), Dtype(-1), params_[i]->gpu_diff(),
                  params_[i]->mutable_gpu_data());
#endif
  }
}

template <typename Dtype>
void Net<Dtype>::ClearParamDiffs()
{
  for (int i = 0; i < params_.size(); ++i)
  {
    Blob<Dtype>* blob = params_[i].get();
#ifdef CPU_ONLY
    caffe_set(blob->count(), Dtype(0), blob->mutable_cpu_diff());
#else
    caffe_gpu_set(blob->count(), Dtype(0), blob->mutable_gpu_diff());
#endif
  }
}

INSTANTIATE_CLASS(Net);

}  // namespace caffe
