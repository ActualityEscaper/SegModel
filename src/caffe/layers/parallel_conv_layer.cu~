#include <vector>

#include "caffe/layer.hpp"
#include "caffe/layers/parallel_conv_layer.hpp"
#include "caffe/util/im2col.hpp"
#include "caffe/util/math_functions.hpp"

namespace caffe {

template <typename Dtype>
void ParallelConvolutionLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*> &top) 
{
//-------------------------------------------------------------------------------------------------------------------------------	
	for (int i = 0; i < bottom.size(); ++i) 
  {  	
  	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
  	ncclBcast((void *)this->parallel_blobs_[i]->mutable_gpu_data(),this->parallel_blobs_[i]->count(),
 																		ncclFloat,0,Caffe::comms(i),Caffe::parallel_stream(i));							
	}	
	for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));			
//--------------------------------------------------------------------------------------------------------------------------------
	int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  int height = bottom[0]->height();
  int width = bottom[0]->width();
 
 	int top_offset_ = height_out_ * width_out_ * num_output_ / group_;
 	int col_offset_ = height_out_ * width_out_ * kernel_size_ * kernel_size_ * channels / group_;
 	int weight_offset_ = kernel_size_ * kernel_size_ * channels * num_output_ / group_ / group_;


	for (int i=0;i<bottom.size();i++)
	{
		const Dtype* bottom_data = bottom[i]->gpu_data();
		Dtype* top_data = top[i]->mutable_gpu_data();
		Dtype* col_data = parallel_col_buffer_[i]->mutable_gpu_data();
		const Dtype* weight = this->blobs_[i]->gpu_data();
		
		

		for (int n = 0; n < num; ++n) 
		{
		  im2col_gpu(bottom_data + bottom[i]->offset(n), channels, height,width, 
		  kernel_size_, kernel_size_, pad_, pad_, stride_, stride_, filter_stride_, filter_stride_, 
		  col_data);   
		  
		  for (int g = 0; g < group_; g++) 
			{
				caffe_gpu_gemm<Dtype>(CblasNoTrans, CblasNoTrans, num_output_/ group_, height_out_*width_out_, kernel_size_*kernel_size_*channels/ group_,
															(Dtype)1., weight+ weight_offset_ * g , col_data + col_offset_ * g,
															(Dtype)0., top_data + top[i]->offset(n) + top_offset_ * g );  
			}												
		}     
	}
//---------------------------------------------------------------------------------------------------------------------------------  
  for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
}

template <typename Dtype>
void ParallelConvolutionLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) 
{	
	int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  int height = bottom[0]->height();
  int width = bottom[0]->width();
  
  int top_offset_ = height_out_ * width_out_ * num_output_ / group_;
 	int col_offset_ = height_out_ * width_out_ * kernel_size_ * kernel_size_ * channels / group_;
 	int weight_offset_ = kernel_size_ * kernel_size_ * channels * num_output_ / group_ / group_;
 	
 	for (int i=0;i<bottom.size();i++)
 	{
	//---------------------------------------------------------------------------------------------------
		parallel_col_buffer_[i]->Reshape(kernel_size_*kernel_size_*channels,height_out_*width_out_,1,1);
	//---------------------------------------------------------------------------------------------------

		const Dtype* top_diff = top[i]->gpu_diff();
		const Dtype* weight = this->parallel_blobs_[i]->gpu_data();
		const Dtype* bottom_data = bottom[i]->gpu_data();
		
		Dtype* bottom_diff = bottom[i]->mutable_gpu_diff();
		Dtype* weight_diff = this->parallel_blobs_[i]->mutable_gpu_diff();
		Dtype* col_data = parallel_col_buffer_[i]->mutable_gpu_data();
		Dtype* col_diff = parallel_col_buffer_[i]->mutable_gpu_diff();

	
		for (int n = 0; n < num; ++n) 
		{
			im2col_gpu(bottom_data + bottom[0]->offset(n), channels, height,width, 
		  kernel_size_, kernel_size_, pad_, pad_, stride_, stride_, filter_stride_, filter_stride_, 
		  col_data);   
		
			for (int g = 0; g < group_; g++) 
			{
				caffe_gpu_gemm<Dtype>(CblasNoTrans, CblasTrans, num_output_ / group_, kernel_size_*kernel_size_*channels / group_, height_out_*width_out_,
															(Dtype)1., top_diff + top[0]->offset(n) + top_offset_ * g, col_data + col_offset_ * g, 
															(Dtype)1., weight_diff + weight_offset_ * g);
			}												
		}
	
		for (int n = 0; n < num; ++n) 
		{
			for (int g = 0; g < group_; g++) 
			{
				caffe_gpu_gemm<Dtype>(CblasTrans, CblasNoTrans, kernel_size_*kernel_size_*channels/ group_, height_out_*width_out_, num_output_/ group_,
															(Dtype)1., weight + weight_offset_ * g, top_diff + top[0]->offset(n) + top_offset_ * g,
															(Dtype)0., col_diff + col_offset_ * g);
			}
		  col2im_gpu(col_diff,  channels, height, width,  
		  kernel_size_, kernel_size_, pad_, pad_, stride_, stride_, filter_stride_, filter_stride_, 
		  bottom_diff + bottom[0]->offset(n));
		}   
	}	
  for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));			
//--------------------------------------------------------------------------------------------------------------------------------  

	for(int i=0;i<bottom.size();i++)
  { 
  	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
  	if (i==0)
			ncclReduce( this->parallel_blobs_[i]->gpu_diff(),this->parallel_blobs_[0]->mutable_gpu_diff(),this->blobs_[0]->count(),
																				ncclFloat,ncclSum,0,Caffe::comms(i),Caffe::parallel_stream(i));
		else
			ncclReduce( this->parallel_blobs_[i]->gpu_diff(),NULL,this->parallel_blobs_[0]->count(),
																				ncclFloat,ncclSum,0,Caffe::comms(i),Caffe::parallel_stream(i));																		
	}
	for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));			
	for(int i=1;i<bottom.size();i++)
  { 
  	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
  	caffe_gpu_set(this->parallel_blobs_[i]->count(),Dtype(0),this->parallel_blobs_[i]->mutable_gpu_diff());
  }	
//----------------------------------------------------------------------------------------------------------------------------------
	for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));			     
}

INSTANTIATE_LAYER_GPU_FUNCS(ParallelConvolutionLayer);

}  // namespace caffe
