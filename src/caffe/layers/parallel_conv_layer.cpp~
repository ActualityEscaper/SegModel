// Copyright 2013 Yangqing Jia

#include <vector>

#include "caffe/layer.hpp"
#include "caffe/layers/parallel_conv_layer.hpp"
#include "caffe/util/im2col.hpp"
#include "caffe/util/math_functions.hpp"

namespace caffe {

template <typename Dtype>
vector<Blob<Dtype> *> ParallelConvolutionLayer<Dtype>::parallel_col_buffer_;


template <typename Dtype>
void ParallelConvolutionLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{
  num_output_ = this->layer_param_.convolution_param().num_output();
  channels_ = bottom[0]->channels();
  kernel_size_ = this->layer_param_.convolution_param().kernel_size();
  pad_ = this->layer_param_.convolution_param().pad();
  stride_ = this->layer_param_.convolution_param().stride();
  filter_stride_ = this->layer_param_.convolution_param().filter_stride();
  group_ = this->layer_param_.convolution_param().group();
  kernel_eff_ = kernel_size_ + (kernel_size_ - 1) * (filter_stride_ - 1);
  CHECK_EQ(channels_%group_,0);
  CHECK_EQ(num_output_%group_,0);
  if (parallel_col_buffer_.size() == 0)
  {
  	parallel_col_buffer_.resize(bottom.size());
  	for (int i=0;i<bottom.size();i++)
  		parallel_col_buffer_[i] = new Blob<Dtype>();
	}
  
  if (this->blobs_.size() > 0)
    LOG(INFO) << "Skipping parameter initialization";
  else
  {  
    this->blobs_.resize(1);


    this->blobs_[0].reset(new Blob<Dtype>(num_output_, channels_/ group_, kernel_size_, kernel_size_));
    shared_ptr<Filler<Dtype> > weight_filler(GetFiller<Dtype>(this->layer_param_.convolution_param().weight_filler()));
    weight_filler->Fill(this->blobs_[0].get());
		
		
		
	
	  if (this->layer_param_.param_size() <= 0)
	  {
	  	this->lr_mult().push_back(1);
	  	this->decay_mult().push_back(1);
	  }	
	  else
	  {
	  	this->lr_mult().push_back(this->layer_param_.param(0).lr_mult());
	  	this->decay_mult().push_back(this->layer_param_.param(0).decay_mult());
	  }
  }
  
  
  parallel_blobs_.resize(bottom.size());  
	parallel_blobs_[0] = this->blobs_[0];	
	for(int i=1;i<bottom.size();i++)
	{
		parallel_blobs_[i].reset(new Blob<Dtype>());
		parallel_blobs_[i]->ReshapeLike(*this->blobs_[0]);
	}
};
template <typename Dtype>
void ParallelConvolutionLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
{

  int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  int height = bottom[0]->height();
  int width = bottom[0]->width();



  height_out_ = (height + 2 * pad_ - kernel_eff_) / stride_ + 1;
  width_out_ = (width + 2 * pad_ - kernel_eff_) / stride_ + 1;
  
  
  for(int i=0;i<bottom.size();i++)
  {
		top[i]->Reshape(num,num_output_,height_out_,width_out_);
		parallel_col_buffer_[i]->Reshape(kernel_size_*kernel_size_*channels,height_out_*width_out_,1,1);
	}

}

template <typename Dtype>
void ParallelConvolutionLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{
}


template <typename Dtype>
void ParallelConvolutionLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) 
{
}

template <typename Dtype>
ParallelConvolutionLayer<Dtype>::~ParallelConvolutionLayer()
{
	for (int i=0;i<parallel_col_buffer_.size();i++)
	{
		if (parallel_col_buffer_[i] != NULL)
		{
			delete parallel_col_buffer_[i];
			parallel_col_buffer_[i] = NULL;
		}
		parallel_col_buffer_.clear();		
	}	
	
	for (int i=0;i<parallel_blobs_.size();i++)
		parallel_blobs_[i].reset();
	parallel_blobs_.clear(); 
}

#ifdef CPU_ONLY
STUB_GPU(ParallelConvolutionLayer);
#endif

INSTANTIATE_CLASS(ParallelConvolutionLayer);
REGISTER_LAYER_CLASS(ParallelConvolution);

}  // namespace caffe
