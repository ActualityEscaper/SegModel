#ifdef USE_CUDNN
#include <vector>

#include "caffe/layers/cudnn_parallel_conv_layer.hpp"

namespace caffe {


__global__ void parallel_sync_conv_groups() { }

template <typename Dtype>
void CuDNNParallelConvolutionLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{  
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));
	this->blobs_[0]->mutable_gpu_data();
	
	for (int i = 0; i < bottom.size(); ++i) 
  {  	
  	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
  	
  	if (i == 0)		
  		caffe_copy(this->blobs_[0]->count(),this->blobs_[0]->gpu_data(),this->parallel_blobs_[0]->mutable_gpu_data());	
  	else
  	{
			CUDA_CHECK(cudaMemcpyPeer(this->parallel_blobs_[i]->mutable_gpu_data(),Caffe::GPUs[i],
																		this->blobs_[0]->gpu_data(),Caffe::GPUs[0],
																		this->blobs_[0]->count()*sizeof(Dtype)
																		));																																																						
		}						
	}
//--------------------------------------------------------------------------------------------------------------------------------


	
  for (int i = 0; i < bottom.size(); ++i) 
  {
  	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));

    CUDNN_CHECK(cudnnConvolutionForward(parallel_handle_[i][0],
          cudnn::dataType<Dtype>::one,
          bottom_descs_, bottom[i]->gpu_data(),
          filter_desc_, this->parallel_blobs_[i]->gpu_data(),
          conv_descs_,
          fwd_algo_, parallel_workspace[i][0], workspace_fwd_sizes_,
          cudnn::dataType<Dtype>::zero,
          top_descs_, top[i]->mutable_gpu_data()));
    parallel_sync_conv_groups<<<1, 1>>>();
  }  
  
//---------------------------------------------------------------------------------------------------------------------------------  
  for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));			
}

template <typename Dtype>
void CuDNNParallelConvolutionLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) 
{
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
	this->buffer_w->mutable_gpu_diff();


//--------------------------------------------------------------------------------------------------------------------------------
  for (int i = 0; i < top.size(); ++i) 
  {
  	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));	
  	
  
    CUDNN_CHECK(cudnnConvolutionBackwardFilter(
          parallel_handle_[i][0],
          cudnn::dataType<Dtype>::one,
          bottom_descs_, bottom[i]->gpu_data(),
          top_descs_,    top[i]->gpu_diff(),
          conv_descs_,
          bwd_filter_algo_, parallel_workspace[i][1], workspace_bwd_filter_sizes_,
          cudnn::dataType<Dtype>::zero,
          filter_desc_, this->parallel_blobs_[i]->mutable_gpu_diff() ));
   

    CUDNN_CHECK(cudnnConvolutionBackwardData(
          parallel_handle_[i][1],
          cudnn::dataType<Dtype>::one,
          filter_desc_, this->parallel_blobs_[i]->gpu_data(),
          top_descs_, top[i]->gpu_diff(),
          conv_descs_,
          bwd_data_algo_, parallel_workspace[i][1], workspace_bwd_data_sizes_,
          cudnn::dataType<Dtype>::zero,
          bottom_descs_, bottom[i]->mutable_gpu_diff()));    
   
    parallel_sync_conv_groups<<<1, 1>>>();
  }



//--------------------------------------------------------------------------------------------------------------------------------  
  // accumulate diff from gpu 0
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
  caffe_gpu_add(this->blobs_[0]->count(),Dtype(1),this->parallel_blobs_[0]->gpu_diff(),Dtype(1),this->blobs_[0]->gpu_diff(),
  																			this->blobs_[0]->mutable_gpu_diff());
	
	
	for(int i=1;i<bottom.size();i++)
  { 
		CUDA_CHECK(cudaMemcpyPeer(this->buffer_w->mutable_gpu_diff(),Caffe::GPUs[0],
														  this->parallel_blobs_[i*2]->gpu_diff(),Caffe::GPUs[i],
															this->blobs_[0]->count()*sizeof(Dtype)
																	));
																																		
		caffe_gpu_add(this->blobs_[0]->count(),Dtype(1),this->buffer_w->gpu_diff(),Dtype(1),this->blobs_[0]->gpu_diff(),
																	this->blobs_[0]->mutable_gpu_diff());
		
												
		CUDA_CHECK(cudaMemcpyPeer(this->buffer_b->mutable_gpu_diff(),Caffe::GPUs[0],
															this->parallel_blobs_[i*2+1]->gpu_diff(),Caffe::GPUs[i],
															this->blobs_[1]->count()*sizeof(Dtype)
																	));														
																	
		caffe_gpu_add(this->blobs_[1]->count(),Dtype(1),this->buffer_b->gpu_diff(),Dtype(1),this->blobs_[1]->gpu_diff(),
																	this->blobs_[1]->mutable_gpu_diff());
	}	
	
	for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));			   
}

INSTANTIATE_LAYER_GPU_FUNCS(CuDNNParallelConvolutionLayer);

}  // namespace caffe
#endif
