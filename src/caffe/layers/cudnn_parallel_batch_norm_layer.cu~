#ifdef USE_CUDNN
#include <vector>

#include "caffe/layers/cudnn_parallel_batch_norm_layer.hpp"

namespace caffe {

template <typename Dtype>
void CuDNNParallelBatchNormLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));
	this->blobs_[0]->mutable_gpu_data();
	this->blobs_[1]->mutable_gpu_data();
	this->blobs_[2]->mutable_gpu_data();
	this->blobs_[3]->mutable_gpu_data();
	this->buffer_w.mutable_gpu_data();
	this->buffer_b.mutable_gpu_data();
	
	double factor;
	if(number_collect_sample == 0)
		factor = 1;
	else
		factor = 0.1;

	for (int i = 0; i < bottom.size(); i++) 
	{  	
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));

		if (i == 0)
		{ 		
			caffe_copy(this->blobs_[0]->count(),this->blobs_[0]->gpu_data(),this->parallel_blobs_[0]->mutable_gpu_data());
			caffe_copy(this->blobs_[1]->count(),this->blobs_[1]->gpu_data(),this->parallel_blobs_[1]->mutable_gpu_data()); 					
			caffe_copy(this->blobs_[2]->count(),this->blobs_[2]->gpu_data(),this->parallel_blobs_[2]->mutable_gpu_data());
			caffe_copy(this->blobs_[3]->count(),this->blobs_[3]->gpu_data(),this->parallel_blobs_[3]->mutable_gpu_data()); 		
		}
		else
		{
			CUDA_CHECK(cudaMemcpyPeer(this->parallel_blobs_[4*i]->mutable_gpu_data(),Caffe::GPUs[i],
																		this->blobs_[0]->gpu_data(),Caffe::GPUs[0],
																		this->blobs_[0]->count()*sizeof(Dtype)
																		));	
			CUDA_CHECK(cudaMemcpyPeer(this->parallel_blobs_[4*i+1]->mutable_gpu_data(),Caffe::GPUs[i],
																		this->blobs_[1]->gpu_data(),Caffe::GPUs[0],
																		this->blobs_[1]->count()*sizeof(Dtype)
																		));	 												
			CUDA_CHECK(cudaMemcpyPeer(this->parallel_blobs_[4*i+2]->mutable_gpu_data(),Caffe::GPUs[i],
																		this->blobs_[2]->gpu_data(),Caffe::GPUs[0],
																		this->blobs_[2]->count()*sizeof(Dtype)
																		));
			CUDA_CHECK(cudaMemcpyPeer(this->parallel_blobs_[4*i+3]->mutable_gpu_data(),Caffe::GPUs[i],
																		this->blobs_[3]->gpu_data(),Caffe::GPUs[0],
																		this->blobs_[3]->count()*sizeof(Dtype)
																		));											
		 }						
	}	
	
	for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		const Dtype* bottom_data = bottom[i]->gpu_data();
		Dtype* top_data = top[i]->mutable_gpu_data();	
		CUDNN_CHECK(
					cudnnBatchNormalizationForwardTraining(parallel_handle_[i],
		      CUDNN_BATCHNORM_SPATIAL,
		      cudnn::dataType<Dtype>::one,cudnn::dataType<Dtype>::zero,
		      parallel_bottom_desc_[i], bottom_data,
		      parallel_top_desc_[i],top_data,
		      parallel_scale_bias_desc_[i],this->parallel_blobs_[4*i]->gpu_data(),this->parallel_blobs_[4*i+1]->gpu_data(),
		      factor,
		      this->parallel_blobs_[4*i+2]->mutable_gpu_data(),this->parallel_blobs_[4*i+3]->mutable_gpu_data(),
		      double(CUDNN_BN_MIN_EPSILON),
		      savedmean[i]->mutable_gpu_data(),savedinvvariance[i]->mutable_gpu_data()
		      ));     
	}	 
	

	// accumulate ave and var to gpu 0
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
	caffe_copy(this->blobs_[2]->count(),this->parallel_blobs_[2]->gpu_data(),this->blobs_[2]->mutable_gpu_data());
	caffe_copy(this->blobs_[3]->count(),this->parallel_blobs_[3]->gpu_data(),this->blobs_[3]->mutable_gpu_data());
  
	for(int i=1;i<bottom.size();i++)
  { 
		CUDA_CHECK(cudaMemcpyPeer(this->buffer_w.mutable_gpu_data(),Caffe::GPUs[0],
																	this->parallel_blobs_[4*i+2]->gpu_data(),Caffe::GPUs[i],
																	this->blobs_[2]->count()*sizeof(Dtype)
																	));
																																		
		caffe_gpu_add(this->blobs_[2]->count(),Dtype(1),this->buffer_w.gpu_data(),Dtype(1),this->blobs_[2]->gpu_data(),this->blobs_[2]->mutable_gpu_data());
		
												
		CUDA_CHECK(cudaMemcpyPeer(this->buffer_b.mutable_gpu_data(),Caffe::GPUs[0],
																	this->parallel_blobs_[4*i+3]->gpu_data(),Caffe::GPUs[i],
																	this->blobs_[3]->count()*sizeof(Dtype)
																	));														
																	
		caffe_gpu_add(this->blobs_[3]->count(),Dtype(1),this->buffer_b.gpu_data(),Dtype(1),this->blobs_[3]->gpu_data(),
																						this->blobs_[3]->mutable_gpu_data());
	}	
	caffe_gpu_scal(this->blobs_[2]->count(),1/Dtype(bottom.size()),this->blobs_[2]->mutable_gpu_data());
	caffe_gpu_scal(this->blobs_[3]->count(),1/Dtype(bottom.size()),this->blobs_[3]->mutable_gpu_data());

	for(int i=0;i<bottom.size();i++)
		number_collect_sample += 1;
 
  for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
              
}

template <typename Dtype>
void CuDNNParallelBatchNormLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) 
{

	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
	this->buffer_w.mutable_gpu_diff();
	this->buffer_b.mutable_gpu_diff();

	
	for(int i=0;i<bottom.size();i++)
	{	
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
	
		const Dtype* top_data = top[i]->gpu_data();
		const Dtype* top_diff = top[i]->gpu_diff();
		const Dtype* bottom_data = bottom[i]->gpu_data();
		Dtype* bottom_diff = bottom[i]->mutable_gpu_diff();
		
		caffe_gpu_set(this->parallel_blobs_[4*i]->count(),Dtype(0),this->parallel_blobs_[4*i]->mutable_gpu_diff());
		caffe_gpu_set(this->parallel_blobs_[4*i+1]->count(),Dtype(0),this->parallel_blobs_[4*i+1]->mutable_gpu_diff());
		
		CUDNN_CHECK(
					cudnnBatchNormalizationBackward(parallel_handle_[i],
					CUDNN_BATCHNORM_SPATIAL,
					cudnn::dataType<Dtype>::one,cudnn::dataType<Dtype>::zero,
					cudnn::dataType<Dtype>::one,cudnn::dataType<Dtype>::one,
					parallel_bottom_desc_[i], bottom_data,
					parallel_top_desc_[i],top_diff,
					parallel_bottom_desc_[i], bottom_diff,
					parallel_scale_bias_desc_[i],this->parallel_blobs_[4*i]->gpu_data(),this->parallel_blobs_[4*i]->mutable_gpu_diff(),this->parallel_blobs_[4*i+1]->mutable_gpu_diff(),
					double(CUDNN_BN_MIN_EPSILON),
					savedmean[i]->gpu_data(),savedinvvariance[i]->gpu_data()
					));     
    	
	} 
	
	// accumulate diff from gpu 0
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	
  caffe_gpu_add(this->blobs_[0]->count(),Dtype(1),this->parallel_blobs_[0]->gpu_diff(),Dtype(1),this->blobs_[0]->gpu_diff(),
  																				this->blobs_[0]->mutable_gpu_diff());
	caffe_gpu_add(this->blobs_[1]->count(),Dtype(1),this->parallel_blobs_[1]->gpu_diff(),Dtype(1),this->blobs_[1]->gpu_diff(),
																					this->blobs_[1]->mutable_gpu_diff());
  

	
	for(int i=1;i<bottom.size();i++)
  { 
		CUDA_CHECK(cudaMemcpyPeer(this->buffer_w.mutable_gpu_diff(),Caffe::GPUs[0],
																	this->parallel_blobs_[4*i]->gpu_diff(),Caffe::GPUs[i],
																	this->blobs_[0]->count()*sizeof(Dtype)
																	));
																																		
		caffe_gpu_add(this->blobs_[0]->count(),Dtype(1),this->buffer_w.gpu_diff(),Dtype(1),this->blobs_[0]->gpu_diff(),
																						this->blobs_[0]->mutable_gpu_diff());
		
												
		CUDA_CHECK(cudaMemcpyPeer(this->buffer_b.mutable_gpu_diff(),Caffe::GPUs[0],
																	this->parallel_blobs_[4*i+1]->gpu_diff(),Caffe::GPUs[i],
																	this->blobs_[1]->count()*sizeof(Dtype)
																	));														
																	
		caffe_gpu_add(this->blobs_[1]->count(),Dtype(1),this->buffer_b.gpu_diff(),Dtype(1),this->blobs_[1]->gpu_diff(),
																						this->blobs_[1]->mutable_gpu_diff());
	}	
	
	for(int i=0;i<bottom.size();i++)
	{
		CUDA_CHECK(cudaSetDevice(Caffe::GPUs[i]));
		CUDA_CHECK(cudaDeviceSynchronize());
	}
	CUDA_CHECK(cudaSetDevice(Caffe::GPUs[0]));	  	
    		  
}

INSTANTIATE_LAYER_GPU_FUNCS(CuDNNParallelBatchNormLayer);

}  // namespace caffe
#endif
