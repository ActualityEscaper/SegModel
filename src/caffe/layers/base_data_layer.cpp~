#include <string>
#include <vector>
#include <boost/thread.hpp>

#include "caffe/layers/base_data_layer.hpp"
#include "caffe/util/io.hpp"

namespace caffe {

template <typename Dtype>
void BaseDataLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{

  data_transformer_.reset( new DataTransformer<Dtype>(transform_param_));
  data_transformer_->InitRand();
  
  
	DataLayerSetUp(bottom, top);
	
  thread_.reset(new boost::thread(&BaseDataLayer<Dtype>::InternalThreadEntry, this));
}

template <typename Dtype>
void BaseDataLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{
}

template <typename Dtype>
void BaseDataLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{
  // wait until the end of the thread
  if (thread_.get() != NULL && thread_->joinable())
  	thread_->join(); 
 
  top[0]->Reshape(prefetch_data_.num(),prefetch_data_.channels(),prefetch_data_.height(),prefetch_data_.width());
  caffe_copy(prefetch_data_.count(), prefetch_data_.cpu_data(),top[0]->mutable_cpu_data());
 
  top[1]->Reshape(prefetch_label_.num(),prefetch_label_.channels(),prefetch_label_.height(),prefetch_label_.width());
  caffe_copy(prefetch_label_.count(), prefetch_label_.cpu_data(), top[1]->mutable_cpu_data());

	CUDA_CHECK(cudaStreamSynchronize(cudaStreamDefault));

  this->data_transformer_->InitRand();
  thread_.reset(new boost::thread(&BaseDataLayer<Dtype>::InternalThreadEntry, this));
}


template <typename Dtype>
void BaseDataLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom)
{
}
template <typename Dtype>
BaseDataLayer<Dtype>::~BaseDataLayer()
{
	if (transformed_data_.count() > 0)
		transformed_data_.set_cpu_data(NULL);
	if (transformed_label_.count() > 0)
		transformed_label_.set_cpu_data(NULL);
}

#ifdef CPU_ONLY
STUB_GPU(BaseDataLayer);
#endif

INSTANTIATE_CLASS(BaseDataLayer);
}  // namespace caffe
