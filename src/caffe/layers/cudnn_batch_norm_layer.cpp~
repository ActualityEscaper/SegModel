#ifdef USE_CUDNN
#include <vector>

#include "caffe/layers/cudnn_batch_norm_layer.hpp"

namespace caffe {

template <typename Dtype>
cudnnHandle_t * CuDNNBatchNormLayer<Dtype>::handle_ = NULL;

template <typename Dtype>
void CuDNNBatchNormLayer<Dtype>::LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
{
 // if (Caffe::phase() == TRAIN && this->layer_param_.batch_norm_param().bn_state() == BNTRAIN)
  {
  	if (handle_ == NULL)
  	{
			handle_ = new cudnnHandle_t;
		  CUDNN_CHECK(cudnnCreate(&handle_[0]));
    }
    
    cudnn::createTensor4dDesc<Dtype>(&bottom_desc_);
    cudnn::createTensor4dDesc<Dtype>(&top_desc_);
    cudnn::createTensor4dDesc<Dtype>(&scale_bias_desc_);
    handles_setup_ = true;
  }   
  if (this->blobs_.size() == 4)
  	LOG(INFO)<<"skip initialization";
  else 
  {
    const int K = bottom[0]->channels();
    this->blobs_.resize(4);
    for(int i=0;i<this->blobs_.size();i++)
    {
      this->blobs_[i].reset(new Blob<Dtype>());
      this->blobs_[i]->Reshape(1,K,1,1);
    }
    caffe_set(this->blobs_[0]->count(),Dtype(1),this->blobs_[0]->mutable_cpu_data());
    caffe_set(this->blobs_[1]->count(),Dtype(0),this->blobs_[1]->mutable_cpu_data());
    caffe_set(this->blobs_[2]->count(),Dtype(0),this->blobs_[2]->mutable_cpu_data());
    caffe_set(this->blobs_[3]->count(),Dtype(1),this->blobs_[3]->mutable_cpu_data());
		
		
		for (int i = 0; i < this->blobs_.size(); i++)
		{
		  if (this->layer_param_.param_size() <= i)
		  {
		  	this->lr_mult().push_back(1);
		  	this->decay_mult().push_back(1);
		  }	
		  else
		  {
		  	this->lr_mult().push_back(this->layer_param_.param(i).lr_mult());
		  	this->decay_mult().push_back(this->layer_param_.param(i).decay_mult());
		  }
		} 
  }

  
  is_initialize = false;
}

template <typename Dtype>
void CuDNNBatchNormLayer<Dtype>::Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
{

  const int N = bottom[0]->num();
  const int K = bottom[0]->channels();
  const int H = bottom[0]->height();
  const int W = bottom[0]->width();

  top[0]->Reshape(N,K,H,W);   
  //if (Caffe::phase() == TRAIN && this->layer_param_.batch_norm_param().bn_state() == BNTRAIN)
  {
    savedmean.Reshape(1,K,1,1);
    savedinvvariance.Reshape(1,K,1,1);

    cudnn::setTensor4dDesc<Dtype>(&bottom_desc_, N, K, H, W);
    cudnn::setTensor4dDesc<Dtype>(&top_desc_, N, K, H, W);
    cudnn::setTensor4dDesc<Dtype>(&scale_bias_desc_,  1, K, 1, 1);
  } 
}
template <typename Dtype>
void CuDNNBatchNormLayer<Dtype>::Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
{}
template <typename Dtype>
void CuDNNBatchNormLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,  const vector<Blob<Dtype>*>& bottom)
{}
template <typename Dtype>
CuDNNBatchNormLayer<Dtype>::~CuDNNBatchNormLayer() 
{
  if (!handles_setup_)
    return;

  cudnnDestroyTensorDescriptor(this->bottom_desc_);
  cudnnDestroyTensorDescriptor(this->top_desc_);
  cudnnDestroyTensorDescriptor(this->scale_bias_desc_);
  
  if (handle_ != NULL)
  {
  	cudnnDestroy(handle_[0]);
  	delete handle_;
  	handle_ = NULL;
  }	
}

INSTANTIATE_CLASS(CuDNNBatchNormLayer);
REGISTER_LAYER_CLASS(CuDNNBatchNorm);
}  // namespace caffe
#endif
