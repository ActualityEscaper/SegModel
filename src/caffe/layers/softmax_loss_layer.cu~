#include <algorithm>
#include <cfloat>
#include <vector>

#include "caffe/layers/softmax_loss_layer.hpp"
#include "caffe/util/math_functions.hpp"

namespace caffe {

template <typename Dtype>
__global__ void SoftmaxLossForwardGPU(const int nthreads,
          const Dtype* prob_data, const Dtype* label, Dtype* loss,
          const int num, const int dim, const int spatial_dim,
          const bool has_ignore_label_, const int ignore_label_,
          Dtype* counts) 
{
  CUDA_KERNEL_LOOP(index, nthreads) 
  {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = static_cast<int>(label[n * spatial_dim + s]);
    if (has_ignore_label_ && label_value == ignore_label_) 
    {
      loss[index] = 0;
      counts[index] = 0;
    } 
    else 
    {
    	if (label_value < 0 || label_value >= dim/spatial_dim)
    	{
    		printf("label %d not valid.\n",label_value);
    		return;
    	}
      loss[index] = -log(max(prob_data[n * dim + label_value * spatial_dim + s], Dtype(FLT_MIN)));
      counts[index] = 1;
    }
  }
}

template <typename Dtype>
__global__ void SoftmaxLossBackwardGPU(const int nthreads, const Dtype* top,
          const Dtype* label, Dtype* bottom_diff, const int num, const int dim,
          const int spatial_dim, const bool has_ignore_label_,
          const int ignore_label_, Dtype* counts) 
{
  const int channels = dim / spatial_dim;

  CUDA_KERNEL_LOOP(index, nthreads) 
  {
    const int n = index / spatial_dim;
    const int s = index % spatial_dim;
    const int label_value = static_cast<int>(label[n * spatial_dim + s]);

    if (has_ignore_label_ && label_value == ignore_label_) 
    {
      for (int c = 0; c < channels; ++c) 
        bottom_diff[n * dim + c * spatial_dim + s] = 0;
        
      counts[index] = 0;
    } 
    else 
    {
      bottom_diff[n * dim + label_value * spatial_dim + s] -= 1;
      counts[index] = 1;
    }
  }
}

template <typename Dtype>
void SoftmaxWithLossLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) 
{

	LOG(INFO)<<bottom[1]->cpu_data()[0];
		
		
  softmax_layer_->Forward(softmax_bottom_vec_, softmax_top_vec_);

  const Dtype* prob_data = prob_.gpu_data();
  const Dtype* label = bottom[1]->gpu_data();

  int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  int height = bottom[0]->height();
  int width = bottom[0]->width();
  
  Dtype* loss_data = loss_.mutable_gpu_data();
  Dtype* count_data = counts_.mutable_gpu_data();
 
  SoftmaxLossForwardGPU<Dtype><<<CAFFE_GET_BLOCKS(num * height * width), CAFFE_CUDA_NUM_THREADS>>>
  (num * height * width, prob_data, label, loss_data, num, channels*height*width, height*width, has_ignore_label_, ignore_label_, count_data);
  
  Dtype loss, count;
  caffe_gpu_asum(num * height * width, loss_data, &loss);
  caffe_gpu_asum(num * height * width, count_data, &count);
  
  //LOG(INFO)<<" num = "<<num<<" height = "<<height<<" width = "<<width;
  
	if (count > 0)
  	top[0]->mutable_cpu_data()[0] = loss / count;
  else
  	top[0]->mutable_cpu_data()[0] = 0; 	 	
}

template <typename Dtype>
void SoftmaxWithLossLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) 
{
  Dtype* bottom_diff = bottom[0]->mutable_gpu_diff();
  const Dtype* prob_data = prob_.gpu_data();
  const Dtype* top_data = top[0]->gpu_data();
  caffe_copy(prob_.count(), prob_data, bottom_diff);
  const Dtype* label = bottom[1]->gpu_data();
	
	int num = bottom[0]->num();
  int channels = bottom[0]->channels();
  int height = bottom[0]->height();
  int width = bottom[0]->width();
  
  Dtype* count_data = counts_.mutable_gpu_data();

  SoftmaxLossBackwardGPU<Dtype><<<CAFFE_GET_BLOCKS(num*height*width), CAFFE_CUDA_NUM_THREADS>>>
  (num*height*width, top_data, label, bottom_diff, num, channels*height*width, height*width, has_ignore_label_, ignore_label_, count_data);

  Dtype count;
  caffe_gpu_asum(num * height * width, count_data, &count);
  
  const Dtype loss_weight = top[0]->cpu_diff()[0] / count;
  caffe_gpu_scal(prob_.count(), loss_weight , bottom_diff);
}

INSTANTIATE_LAYER_GPU_FUNCS(SoftmaxWithLossLayer);

}  // namespace caffe
