// Copyright 2013 Yangqing Jia

#ifndef CAFFE_LAYER_H_
#define CAFFE_LAYER_H_

#include <vector>
#include "caffe/blob.hpp"
#include "caffe/layer_factory.hpp"
#include "caffe/common.hpp"
#include "caffe/proto/caffe.pb.h"
#include "caffe/util/math_functions.hpp"

using std::vector;

namespace caffe {

template <typename Dtype>
class Layer
{
 public:
  explicit Layer(const LayerParameter& param): layer_param_(param) 
  {
  	lr_mult_.clear();
		decay_mult_.clear();
  	for(int i=0;i<param.param_size();i++)
  	{
  		lr_mult_.push_back(param.param(i).lr_mult());
			decay_mult_.push_back(param.param(i).decay_mult());
  	}
		loss_weight_ = param.include().loss_weight();
  }
  virtual ~Layer();

  
  void SetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)  
  {     
  	LayerSetUp(bottom, top); 
  	Reshape(bottom, top); 
  }
  
  inline Dtype Forward(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top);
  inline void Backward(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom);


  vector<shared_ptr<Blob<Dtype> > >& blobs() { return blobs_; }
	vector<Dtype>& lr_mult() { return lr_mult_; }
	vector<Dtype>& decay_mult() { return decay_mult_; }
	

  const LayerParameter& layer_param() { return layer_param_; }
  virtual void ToProto(LayerParameter* param, bool write_diff = false);
	
	
	virtual void LayerSetUp(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) = 0;
	virtual void Reshape(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) = 0;
  virtual void Forward_cpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) = 0;
  virtual void Backward_cpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) = 0;
  
  virtual void Forward_gpu(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) = 0;
  virtual void Backward_gpu(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) = 0;
	
 protected:
  LayerParameter layer_param_;
  vector<Dtype> lr_mult_;
	vector<Dtype> decay_mult_;
	Dtype loss_weight_;
  vector<shared_ptr<Blob<Dtype> > > blobs_;

	static vector<Blob<Dtype> *> workspace_;
	
  DISABLE_COPY_AND_ASSIGN(Layer);
};  

template <typename Dtype>
vector<Blob<Dtype> *> Layer<Dtype>::workspace_;

template <typename Dtype>
Layer<Dtype>::~Layer()
{
 for (int i = 0; i < workspace_.size();i++)
 {
 		if (workspace_[i] != NULL)
 		{
 			delete workspace_[i];
 			workspace_[i] = NULL;
 		}	
 }
 workspace_.clear();
}

template <typename Dtype>
inline Dtype Layer<Dtype>::Forward(const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top)
{
#ifdef CPU_ONLY
	Forward_cpu(bottom, top);
#else
	Forward_gpu(bottom, top);
#endif
	Dtype loss;
	if (loss_weight_ > 0)
		loss = top[0]->cpu_data()[0] * loss_weight_;
	else
		loss = 0;

	return loss;
};

template <typename Dtype>
inline void Layer<Dtype>::Backward(const vector<Blob<Dtype>*>& top, const vector<Blob<Dtype>*>& bottom) 
{
		if (loss_weight_ > 0)
		{
			CHECK_EQ(top.size(),1)<<"there should only one top in the loss layer";
			top[0]->mutable_cpu_diff()[0] = loss_weight_;
		}
#ifdef CPU_ONLY
    Backward_cpu(top, bottom);
#else
    Backward_gpu(top, bottom);
#endif
};

//----------------------------------------- proto <->  memory--------------------
template <typename Dtype>
void Layer<Dtype>::ToProto(LayerParameter* param, bool write_diff) 
{
  param->Clear();
  param->CopyFrom(layer_param_);
  param->clear_blobs();
  for (int i = 0; i < blobs_.size(); ++i) 
    blobs_[i]->ToProto(param->add_blobs(), write_diff);
  
}
//---------------------------------------------------------------------------------------
}  // namespace caffe

#endif  // CAFFE_LAYER_H_
